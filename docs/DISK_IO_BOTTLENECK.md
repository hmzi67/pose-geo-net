# Disk I/O Bottleneck Analysis & Solutions

## üîç Problem Identified

**Current Status:**
- GPU Utilization: **28%** (Target: 90-100%)
- Memory Usage: 5 GB / 24 GB
- Power Draw: 133W (Target: 300-350W)

**Root Cause:** **Disk I/O Bottleneck**
- Loading 104,052 images from disk during training
- Each epoch reads ~104K images from storage
- Workers are waiting for disk reads ‚Üí GPU starves

## üìä Bottleneck Analysis

```
Training Loop Flow:
1. DataLoader workers read images from disk  ‚Üê BOTTLENECK HERE
2. Apply transforms (resize, normalize)
3. Transfer to GPU
4. Forward pass                              ‚Üê GPU UNDERUTILIZED
5. Backward pass                             ‚Üê GPU UNDERUTILIZED
```

**Why GPU is only 28% utilized:**
- GPU processes batches in ~0.1s
- Disk takes ~0.3-0.5s to load next batch
- GPU waits 70% of the time doing nothing

## ‚úÖ Solutions Implemented

### 1. Increased Workers (12 ‚Üí Parallel Loading)
```python
num_workers=12          # More parallel loaders
prefetch_factor=8       # Load 8 batches ahead
```

### 2. Optimized Image Loading
```python
cv2.imread(path, cv2.IMREAD_COLOR)  # Faster loading
```

### 3. Persistent Workers
```python
persistent_workers=True  # Don't recreate workers each epoch
```

## üöÄ Additional Solutions (If Still Slow)

### Solution A: **Use SSD Instead of HDD**
```bash
# Check if data is on SSD or HDD
df -Th /home/genesys/hamza/cervical/Archive/data/300W_LP

# If on HDD, move to SSD
sudo rsync -av --progress /path/to/hdd/300W_LP /path/to/ssd/300W_LP
```

**Expected improvement:** 3-5x faster I/O

### Solution B: **Copy Dataset to /tmp (RAM Disk)**
```bash
# Copy dataset to tmpfs (RAM disk) - FAST!
sudo mkdir -p /tmp/300W_LP
sudo rsync -av --progress data/300W_LP/ /tmp/300W_LP/

# Update paths in preprocessing script
# Then train with data in RAM
```

**Expected improvement:** 10x faster I/O, but uses ~10-15 GB RAM

### Solution C: **Image Caching (First Epoch Slow, Rest Fast)**

Create a cached dataset loader:

```python
class CachedImageDataset(Dataset):
    def __init__(self, image_paths, labels, cache_size=10000):
        self.image_paths = image_paths
        self.labels = labels
        self.cache = {}
        self.cache_size = cache_size
        
    def __getitem__(self, idx):
        if idx in self.cache:
            img = self.cache[idx]
        else:
            img = cv2.imread(self.image_paths[idx])
            if len(self.cache) < self.cache_size:
                self.cache[idx] = img
        # ... rest of processing
```

**Expected improvement:** First epoch slow, subsequent epochs 5x faster

### Solution D: **Preload Images to Memory** (Best but needs 32+ GB RAM)

```bash
# Preprocess all images into a single .npy file
python preprocess_to_memory.py

# This creates one large file: train_images.npy (20-30 GB)
# Then training loads from memory instead of disk
```

**Expected improvement:** 10-20x faster, GPU at 90-100%

### Solution E: **NVIDIA DALI (Professional Solution)**

Install NVIDIA DALI for GPU-accelerated data loading:

```bash
pip install nvidia-dali-cuda120

# DALI does image decoding on GPU
# Frees up CPU and disk I/O
```

**Expected improvement:** 5-10x faster, GPU at 95-100%

## üéØ Recommended Approach

### Immediate (No Code Change):
1. **Check if dataset is on SSD**
   ```bash
   df -Th data/300W_LP
   ```
   - If HDD ‚Üí Move to SSD
   - If SSD ‚Üí Proceed to next step

2. **Increase batch size** (more work per load)
   ```bash
   python train_cnn_model.py --dataset 300wlp --epochs 50 --batch-size 256
   ```

3. **Monitor during training**
   ```bash
   # Terminal 1: Train
   python train_cnn_model.py --dataset 300wlp --epochs 50 --batch-size 256
   
   # Terminal 2: Monitor
   python monitor_gpu.py
   ```

### Short-term (Best ROI):
**Copy dataset to /tmp (if you have 32+ GB RAM)**

```bash
# 1. Check RAM available
free -h

# 2. If you have >32 GB free, copy to RAM disk
cp -r data/300W_LP /tmp/

# 3. Update preprocessing script to use /tmp/300W_LP

# 4. Train (will be MUCH faster)
python train_cnn_model.py --dataset 300wlp --epochs 50 --batch-size 256
```

### Long-term (Best Performance):
**Preprocess dataset once, save to .npy**

This loads ALL images into memory once:
```bash
# Create preprocessed dataset (run once)
python create_memory_dataset.py

# Train (very fast, GPU at 95-100%)
python train_cnn_model_fast.py --dataset 300wlp --epochs 50 --batch-size 256
```

## üìà Expected Results

### Current (Disk I/O Bottleneck):
- GPU: 28%
- Speed: ~1.1s/batch
- Epoch time: ~7-8 minutes
- **Full training (50 epochs): ~6-7 hours**

### With SSD:
- GPU: 60-70%
- Speed: ~0.4s/batch
- Epoch time: ~3 minutes
- **Full training (50 epochs): ~2.5 hours**

### With RAM Disk or Memory Dataset:
- GPU: **90-100%** ‚úÖ
- Speed: ~0.1s/batch
- Epoch time: ~40 seconds
- **Full training (50 epochs): ~35 minutes** ‚úÖ

## üîß Quick Diagnostic

Run this to check your current bottleneck:

```bash
# While training is running:
# 1. Check disk I/O
iostat -x 2

# High %util means disk bottleneck
# Look for:
#   %util > 90% = Disk bottleneck
#   %util < 50% = Not disk bottleneck

# 2. Check if data is on SSD or HDD
lsblk -o NAME,TYPE,SIZE,MODEL,ROTA
# ROTA=1 means HDD (spinning disk - SLOW)
# ROTA=0 means SSD (solid state - FAST)
```

## üí° Quick Wins

Even without major changes, these help:

1. **Reduce image size** (if acceptable)
   - Currently: 224x224
   - Try: 192x192 or 160x160
   - Less data to load, faster processing

2. **Reduce color jitter** (faster transforms)
   ```python
   # In data_preprocessing_cnn.py, reduce or remove ColorJitter
   ```

3. **Use JPEG instead of PNG** (if applicable)
   - JPEGs load ~2x faster

4. **Disable augmentation for testing**
   - See actual model speed without I/O bottleneck

## üìù Summary

**Current Performance:**
- Training ~1.1s per batch
- 407 batches per epoch
- ~7-8 minutes per epoch
- **GPU only 28% utilized**

**After Optimizations (with SSD/RAM):**
- Training ~0.1-0.2s per batch
- 407 batches per epoch  
- ~1 minute per epoch
- **GPU 90-100% utilized**

**Bottom Line:**
Your GPU is powerful but starving for data. Move dataset to faster storage (SSD or RAM) for best results!

---

## üéØ Action Items

1. ‚úÖ Increased workers to 12 (done)
2. ‚úÖ Increased prefetch to 8 (done)
3. ‚è≥ Check if dataset on SSD
4. ‚è≥ Consider copying to /tmp if RAM available
5. ‚è≥ Let current training finish to see improvements

**Current training will be slow (~6-7 hours), but subsequent runs with these optimizations should be much faster!**
